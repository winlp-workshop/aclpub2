Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages. In this talk, I would describe different approaches to scaling evaluation to several languages. First, I would describe simple strategies for extending multilingual evaluations by re-purposing existing English datasets to over 200 languages for both text (SIB-200) and speech modalities (Fleurs-SLU). Second, I would introduce our recent bench IrokoBench -- a human-translated benchmark dataset for 17 typologically-diverse low-resource African languages covering three tasks: natural language inference, mathematical reasoning, and multi-choice knowledge-based question answering. This evaluation expands the evaluation of many low-resource languages from simple text classification tasks to more challenging knowledge and reasoning tasks. We observe a significant performance gap between open and proprietary models, with the highest performing open model, Gemma 2 27B, only at 60\% of the best-performing proprietary model GPT-4o performance. These findings suggest that more efforts are needed to develop and adapt LLMs for low-resource languages. Finally, I will highlight some of our recent projects that make some of these challenging datasets more multicultural for Visual question answering and intent detection tasks, to encourage practical usage of LLMs within the low-resource communities.