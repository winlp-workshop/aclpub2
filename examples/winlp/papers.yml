- abstract: 'Vision-Language Models (VLMs) often appearculturally competent but rely
    on superficial pat.tern matching rather than genuine cultural understanding. We
    introduce a diagnostic framework to probe VLM reasoning on fire-themedcultural
    imagery through both classification andexplanation analysis. Testing multiple
    modelson Western festivals, non-Western traditions.and emergency scenes reveals
    systematic biases: models correctly identify prominent Western festivals but struggle
    with underrepresentedcultural events, frequently offering vague labelsor dangerously
    misclassifying emergencies ascelebrations. These failures expose the risksof symbolic
    shortcuts and highlight the needfor cultural evaluation beyond accuracy metrics
    to ensure interpretable and fair multimodalsystems.'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@dundee.ac.uk'
    first_name: Haorui
    institution: University of Dundee
    last_name: Yu
    name: Haorui Yu
    orcid: https://orcid.org/0009-0001-2087-8879
    username: ~Haorui_Yu1
  - emails: '****@gmail.com'
    first_name: Yang
    last_name: Zhao
    name: Yang Zhao
    orcid: https://orcid.org/0009-0006-8952-1380
    username: ~Yang_Zhao47
  - emails: '****@stu.xmu.edu.cn'
    first_name: Yijia
    last_name: Chu
    name: Yijia Chu
    orcid: https://orcid.org/0009-0003-0414-7819
    username: ~Yijia_Chu1
  - emails: '****@student.bham.ac.uk'
    first_name: Qiufeng
    last_name: Yi
    name: Qiufeng Yi
    orcid: https://orcid.org/0000-0002-5611-5769
    username: ~Qiufeng_Yi1
  decision: '2025'
  file: 1.pdf
  id: 1
  openreview_id: yOnvBDv7zL
  pdf_file: 7e93c6b111a750a698cbd8743867ec22490df11d.pdf
  title: 'Seeing Symbols, Missing Cultures: Probing Vision-Language Models'' Reasoning
    on Fire Imagery and Cultural Meaning'
- abstract: 'Abstract Meaning Representation (AMR) is a graph-based semantic representation
    that has been incorporated into numerous downstream tasks, in particular due to
    substantial efforts developing text-to-AMR parsing and AMR-to-text generation
    models. However, there still exists a large gap between fluent, natural sentences
    and texts generated from AMR-to-text generation models. Prompt-based Large Language
    Models (LLMs), on the other hand, have demonstrated an outstanding ability to
    produce fluent text in a variety of languages and domains. In this paper, we investigate
    the extent to which LLMs can improve the AMR-to-text generated output fluency
    post-hoc via prompt engineering. We conduct automatic and human evaluations of
    the results, and ultimately have mixed findings: LLM-generated paraphrases generally
    do not exhibit improvement in automatic evaluation, but outperform baseline texts
    according to our human evaluation. Thus, we provide a detailed error analysis
    of our results to investigate the complex nature of generating highly fluent text
    from semantic representations.'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@amherst.edu'
    first_name: Jiyuan
    last_name: Ji
    name: Jiyuan Ji
    username: ~Jiyuan_Ji1
  - dblp_id: https://dblp.org/pid/263/2502
    emails: '****@gmail.com'
    first_name: Shira
    homepage: https://shirawein.github.io
    institution: Amherst College
    last_name: Wein
    name: Shira Wein
    orcid: https://orcid.org/0000-0002-1062-0866
    username: ~Shira_Wein1
  decision: '2025'
  file: 4.pdf
  id: 4
  openreview_id: pK1lfP0AMA
  pdf_file: 29c279ed98fa0862dbc26c087a5b3909ad2bc3df.pdf
  title: 'GPT4AMR: Does LLM-based Paraphrasing Improve AMR-to-text Generation Fluency?'
- abstract: Multilingual Large Language Models (LLMs) are increasingly used worldwide,
    making it essential to ensure they are free from gender bias to prevent representational
    harm. While prior studies have examined such biases in high-resource languages,
    low-resource languages remain understudied. In this paper, we propose a template-based
    probing methodology, validated against real-world data, to uncover gender stereotypes
    in LLMs. As part of this framework, we introduce the Domain-Specific Gender Skew
    Index (DS-GSI), a metric that quantifies deviations from gender parity. We evaluate
    four prominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ
    32B, across four semantic domains, focusing on Persian, a low-resource language
    with distinct linguistic features. Our results show that all models exhibit gender
    stereotypes, with greater disparities in Persian than in English across all domains.
    Among these, sports reflect the most rigid gender biases. This study underscores
    the need for inclusive NLP practices and provides a framework for assessing bias
    in other low-resource languages.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/318/8551.html
    emails: '****@illinois.edu'
    first_name: Ghazal
    google_scholar_id: https://scholar.google.com/citations?user=hhMHtcUAAAAJ&hl=en
    institution: University of Tehran, University of Tehran
    last_name: Kalhor
    name: Ghazal Kalhor
    orcid: https://orcid.org/0000-0002-6153-9048
    semantic_scholar_id: https://www.semanticscholar.org/author/Ghazal-Kalhor/2161768554
    username: ~Ghazal_Kalhor3
  - dblp_id: https://dblp.org/pid/52/3356
    emails: '****@teias.institute'
    first_name: Behnam
    google_scholar_id: https://scholar.google.com/citations?user=1IdcoLMAAAAJ&hl=en
    institution: Tehran Institute for Advanced Studies
    last_name: Bahrak
    name: Behnam Bahrak
    orcid: https://orcid.org/0000-0003-4429-2511
    semantic_scholar_id: https://www.semanticscholar.org/author/B.-Bahrak/2860215
    username: ~Behnam_Bahrak2
  decision: '2025'
  file: 9.pdf
  id: 9
  openreview_id: axtJnsarto
  pdf_file: 570cbb62692f4aeff8f2b1e0e218e954bf32c4b0.pdf
  title: 'Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in
    Persian'
- abstract: 'Recent works have analyzed the impact of individual components of neural
    networks on gendered predictions, often with a focus on mitigating gender bias.
    However, mechanistic interpretations of gender tend to (i) focus on a very specific
    gender-related task, such as gendered pronoun prediction, or (ii) fail to distinguish
    between the production of \textit{factually gendered outputs} (the correct assumption
    of gender given a word that carries gender as a semantic property) and \textit{gender
    biased outputs} (based on a stereotype). To address these issues, we curate GKnow,
    a benchmark to assess gender knowledge and gender bias in autoregressive models
    across different types of gender-related predictions. GKnow allows us to identify
    and analyze the neurons responsible for gendered predictions. We test the impact
    of neuron ablation on benchmarks for disentangling stereotypical and factual gender
    (DiFair and the test set of \gknow), as well as StereoSet. Results show that gender
    bias and factual gender are

    severely entangled on a neuron level, indicating that neuron

    ablation is an unreliable debiasing method. Furthermore,

    benchmarks for evaluating gender bias can hide the decrease

    in factual gender knowledge that accompanies neuron

    ablation. We curate GKnow as a contribution to the

    continuous development of robust gender bias benchmarks.'
  archival: false
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@cis.lmu.de'
    first_name: Leonor
    homepage: https://www.linkedin.com/in/leonor-veloso/
    institution: Ludwig-Maximilians-Universität München
    last_name: Veloso
    name: Leonor Veloso
    username: ~Leonor_Veloso1
  - dblp_id: https://dblp.org/pid/s/HinrichSchutze
    emails: '****@cis.lmu.de'
    first_name: Hinrich
    homepage: https://www.cis.uni-muenchen.de/schuetze/
    last_name: Schuetze
    name: Hinrich Schuetze
    username: ~Hinrich_Schuetze3
  decision: '2025'
  file: 10.pdf
  id: 10
  openreview_id: Ju0i3qQTS0
  pdf_file: 3ad9b5e72fc2cc88b7877b85fcd578648df423f2.pdf
  title: 'GKnow: A Dataset for Measuring the Neuron-Level Entanglement of Gender Bias
    and Factual Gender'
- abstract: An automatic court hearing transcription system is being developed for
    the Federal Supreme Court of Ethiopia to address the challenges faced in manual
    transcription. By utilizing Automatic Speech Recognition technology, the system
    aims to transcribe Amharic language court recordings accurately and efficiently.
    This innovative solution not only improves the court system but also safeguards
    the health of transcribers and enhances the overall speed and quality of legal
    proceedings in Ethiopia. In this study, a self-supervised Transformer based Wave2Vec
    2.0 approach has been conducted to build an ASR system. With a dataset comprising
    over 500 hours of unlabeled data, the system has achieved a remarkable Word Error
    Rate (WER) of 14.36%, showcasing its effectiveness in transcribing court proceedings
    with high accuracy.
  archival: false
  attributes:
    paper_length: Extended Abstract (2 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@gmail.com'
    first_name: Abel
    institution: Arba Minch University
    last_name: Alemu
    middle_name: Mulat
    name: ABEL MULAT ALEMU
    username: ~ABEL_MULAT_ALEMU1
  - emails: '****@gmail.com'
    first_name: Rahel
    last_name: Tamiru
    middle_name: Mekonen
    name: Rahel Mekonen Tamiru
    username: ~Rahel_Mekonen_Tamiru1
  decision: '2025'
  file: 11.pdf
  id: 11
  openreview_id: VOp4qMC5uN
  pdf_file: 997bb22b3db64ce28771cb84ec9123d6da186a7d.pdf
  title: Wav2Vec-Based Self-Supervised Learning for Court Hearing Transcription
- abstract: Most resources for evaluating social biases in Large Language Models are
    developed without co-design from the communities affected by these biases, and
    rarely involve participatory approaches. We introduce HESEIA, a dataset of 46,499
    sentences created in a teacher professional development course. The course involved
    370 high-school teachers and 5,370 students from 189 Latin-American schools. Unlike
    existing benchmarks, HESEIA captures intersectional biases across multiple demographic
    axes and school subjects. It reflects local contexts through the lived experience
    and pedagogical expertise of educators. Teachers used minimal pairs to create
    sentences that express stereotypes relevant to their school subjects and communities.
    We show the dataset diversity in term of the types of biases represented and also
    in terms of the knowledge areas included. We demonstrate that the dataset contains
    more stereotypes unrecognized by current LLMs than previous datasets, potentially
    making bias mitigation by self-debiasing harder. HESEIA is available to support
    bias assessments grounded in educational communities.
  archival: false
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/375/8426
    emails: '****@mi.unc.edu.ar'
    first_name: Guido
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=_QA3Hs4AAAAJ
    institution: Universidad Nacional de Córdoba
    last_name: Ivetta
    name: Guido Ivetta
    orcid: https://orcid.org/0009-0006-3248-1461
    semantic_scholar_id: https://www.semanticscholar.org/author/Guido-Ivetta/2213060824
    username: ~Guido_Ivetta1
  - emails: '****@unc.edu.ar'
    first_name: Marcos
    google_scholar_id: https://scholar.google.com.ar/citations?user=FTHzyOEAAAAJ&hl=en
    last_name: Gomez
    middle_name: J
    name: Marcos J Gomez
    username: ~Marcos_J_Gomez1
  - emails: '****@mi.unc.edu.ar'
    first_name: Sofía
    last_name: Martinelli
    name: Sofía Martinelli
    username: ~Sofía_Martinelli1
  - emails: '****@mi.unc.edu.ar'
    first_name: Pietro
    last_name: Palombini
    name: Pietro Palombini
    username: ~Pietro_Palombini1
  - emails: '****@unc.edu.ar'
    first_name: M
    google_scholar_id: https://scholar.google.es/citations?user=T2IZhO4AAAAJ&hl=es
    homepage: https://www.conicet.gov.ar/new_scp/detalle.php?id=46531&datos_academicos=yes
    institution: Universidad Nacional de Córdoba
    last_name: Echeveste
    middle_name: Emilia
    name: M Emilia Echeveste
    orcid: https://orcid.org/0000-0001-8338-0384
    username: ~M_Emilia_Echeveste1
  - emails: '****@gmail.com'
    first_name: Nair
    homepage: https://ia.vialibre.org.ar/
    institution: Fundación Vía Libre
    last_name: Mazzeo
    middle_name: Carolina
    name: Nair Carolina Mazzeo
    username: ~Nair_Carolina_Mazzeo2
  - emails: '****@uba.ar'
    first_name: Beatriz
    institution: Universidad Torcuato di Tella and Universidad de Buenos Aires
    last_name: Busaniche
    name: Beatriz Busaniche
    orcid: https://orcid.org/0009-0005-9932-3022
    username: ~Beatriz_Busaniche1
  - dblp_id: https://dblp.org/pid/22/1403
    emails: '****@unc.edu.ar'
    first_name: Luciana
    google_scholar_id: https://scholar.google.com/citations?user=MHvcOG0AAAAJ&hl=es
    homepage: https://benotti.github.io/
    institution: 'Universidad nacional de Córdoba '
    last_name: Benotti
    name: Luciana Benotti
    orcid: https://orcid.org/0000-0001-7456-4333
    semantic_scholar_id: https://www.semanticscholar.org/author/Luciana-Benotti/3131683?sort=pub-date
    username: ~Luciana_Benotti1
  decision: '2025'
  file: 12.pdf
  id: 12
  openreview_id: zEsBIsiaSO
  pdf_file: 3bb9b6bdcd616cbc6c2c93949bca7c3bc13fd8f7.pdf
  title: An intersectional bias evaluation dataset grounded in educational contexts
- abstract: In this study, we investigate how author affiliation shapes academic discourse,
    proposing it as an effective proxy for author perspective in understanding what
    topics are studied, how nations are framed, and whose realities are prioritised.
    Using Palestine as a case study, we apply BERTopic and Structural Topic Modelling
    (STM) to 29,536 English-language academic articles collected from the OpenAlex
    database. We find that domestic authors focus on practical, local issues like
    healthcare, education, and the environment, while foreign authors emphasise legal,
    historical, and geopolitical discussions. These differences, in our interpretation,
    reflect lived proximity to war and crisis. We also note that while BERTopic captures
    greater lexical nuance, STM enables covariate-aware comparisons, offering deeper
    insight into how affiliation correlates with thematic emphasis. We propose extending
    this framework to other underrepresented countries, including a future study focused
    on Gaza post-October 7.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@kaist.ac.kr'
    first_name: Maida
    institution: Korea Advanced Institute of Science & Technology
    last_name: Aizaz
    name: Maida Aizaz
    username: ~Maida_Aizaz1
  - emails: '****@kaist.ac.kr'
    first_name: Taegyoon
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&authuser=2&user=QRphKMcAAAAJ&gmla=AJsN-F42Bgxo9pb7FriSFnrwWKpfRcwajhi9gxcktX2Wh2uaZrjhjhfUTvNgoiF4gyABtOVFgefdcKT7e2gg8I3arMKH3cnzFeGBBeuj4uGJDECko3sf1eZdiGrHiuonxABXIHz5BW2_
    homepage: https://taegyoon-kim.github.io
    institution: KAIST
    last_name: Kim
    name: Taegyoon Kim
    username: ~Taegyoon_Kim2
  - emails: '****@kaist.ac.kr'
    first_name: Lanu
    google_scholar_id: https://scholar.google.com/citations?user=77i0fdMAAAAJ&hl=en&oi=ao
    homepage: https://lanukim.github.io
    institution: Korea Advanced Institute of Science & Technology
    last_name: Kim
    name: Lanu Kim
    orcid: https://orcid.org/0000-0002-7381-4959
    username: ~Lanu_Kim1
  decision: '2025'
  file: 13.pdf
  id: 13
  openreview_id: LdAeTi9f6o
  pdf_file: ac6e66f23822fcb034be889588f6db0570ac7f31.pdf
  title: Whose Palestine Is It? A Topic Modelling Approach to National Framing in
    Academic Research
- abstract: Named Entity Recognition (NER) is the information extraction task of identifying
    predefined named entities such as person names, location names, organization names
    and more. High-resource languages have made significant progress in NER tasks.
    However, low-resource languages such as Kurmanji Kurdish have not seen the same
    advancements, due to these languages having less available data online. This research
    aims to close this gap by developing an NER system via fine-tuning XLM-RoBERTa
    on a manually annotated dataset for Kurmanji. The dataset used for fine-tuning
    consists of 7,919 annotated sentences, which were manually annotated by three
    native Kurmanji speakers. The classes labeled in the dataset are Person (PER),
    Organization (ORG), and Location (LOC). A web-based application has also been
    developed using Streamlit to make the model more accessible. The model achieved
    an F1 score of 0.8735, precision of 0.8668, and recall of 0.8803, demonstrating
    the effectiveness of fine-tuning transformer-based models for NER tasks in low-resource
    languages. This work establishes a methodology that can be applied to other low-resource
    languages and Kurdish varieties.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@rgu.ac.uk'
    first_name: Hossein
    google_scholar_id: https://scholar.google.com/citations?user=SoYcEooAAAAJ&hl=en
    homepage: https://www.ukh.edu.krd/faculty/hossein-hassani/
    institution: University of Kurdistan Hewlêr
    last_name: Hassani
    name: Hossein Hassani
    orcid: https://orcid.org/0000-0002-8899-4016
    username: ~Hossein_Hassani2
  decision: '2025'
  file: 16.pdf
  id: 16
  openreview_id: 8VRf9YoGm9
  pdf_file: 1f2ce6430a765024353856bee9cd31dc6736e8a3.pdf
  title: Fine-tuning XLM-RoBERTa for Named Entity Recognition in Kurmanji Kurdish
- abstract: "Capability evaluation of large language models (LLMs) is increasingly\
    \ complicated by rising concerns of data contamination that cast doubts on whether\
    \ static benchmarks measure genuine reasoning or mere memorization.\nTo move beyond\
    \ publicly sourced questions, we introduce a fully automated, infinitely scalable\
    \ framework that synthesizes research-level questions directly from arXiv papers,\
    \ harnessing the natural temporal structure of research publications where performance\
    \ decay after knowledge cutoffs may indicate potential contamination. \nWe evaluated\
    \ 8 frontier models from 4 families with different knowledge cutoff dates on 1,643\
    \ multi-step reasoning questions synthesized from 20,277 arXiv papers stratified\
    \ monthly over 2 years, covering at least 6 months before and after their knowledge\
    \ cutoff dates. \nOur results consistently showed no significant performance decay\
    \ near knowledge cutoff dates for models of various sizes, developers, and release\
    \ dates. \nBy comparing with other longitudinal studies that retrieve public questions\
    \ directly and observe significant post-cutoff performance decay, we hypothesize\
    \ that the multi-step reasoning required by our synthesis pipeline mitigates contamination\
    \ by demanding deeper understanding than shallow pattern-matching. \nWe validate\
    \ that reasoning-driven synthesis from research papers serves as both a robust\
    \ capability probe and an effective mitigation strategy against benchmark contamination,\
    \ enabling more faithful assessment of genuine reasoning capabilities at scale.\n\
    We will open source our code and datasets after peer review to facilitate reproducibility\
    \ and promote the adoption of contamination-resistant evaluation methods."
  archival: false
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/408/5364
    emails: '****@ethz.ch'
    first_name: Terry
    google_scholar_id: https://scholar.google.co.uk/citations?hl=en&user=hFY4t8gAAAAJ
    homepage: https://terry-zhang.notion.site/
    last_name: Zhang
    middle_name: Jingchen
    name: Terry Jingchen Zhang
    orcid: https://orcid.org/0009-0009-1271-4696
    semantic_scholar_id: https://www.semanticscholar.org/author/Terry-Jingchen-Zhang/2363686592
    username: ~Terry_Jingchen_Zhang1
  - emails: '****@ee.iitr.ac.in'
    first_name: Gopal
    institution: Max-Planck Institute
    last_name: Dev
    name: Gopal Dev
    username: ~Gopal_Dev1
  - emails: '****@ethz.ch'
    first_name: Ning
    homepage: https://ethz.ch/de.html
    last_name: Wang
    name: Ning Wang
    username: ~Ning_Wang33
  - emails: '****@seas.upenn.edu'
    first_name: Nicole
    last_name: Ni
    name: Nicole Ni
    username: ~Nicole_Ni1
  - emails: '****@student.ethz.ch'
    first_name: Wenyuan
    homepage: https://github.com/jwyjohn
    institution: ETHZ - ETH Zurich
    last_name: Jiang
    name: Wenyuan Jiang
    orcid: https://orcid.org/0000-0003-4646-7960
    username: ~Wenyuan_Jiang4
  - dblp_id: https://dblp.org/pid/229/4267
    emails: '****@cs.toronto.edu'
    first_name: Zhijing
    google_scholar_id: https://scholar.google.com/citations?user=Mdr6wjUAAAAJ&hl=en
    homepage: https://zhijing-jin.com
    institution: Department of Computer Science, University of Toronto
    last_name: Jin
    name: Zhijing Jin
    orcid: https://orcid.org/0000-0003-0238-9024
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhijing-Jin/8752221
    username: ~Zhijing_Jin1
  decision: '2025'
  file: 17.pdf
  id: 17
  openreview_id: r95XbeSspr
  pdf_file: e63a13c1cc784f08e2cfc33c98da697d331c2557.pdf
  title: 'Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy
    Against Benchmark Contamination'
- abstract: As Large Language Models (LLMs) are deployed in every aspect of our lives,
    understanding how they reason about moral issues becomes critical for AI safety.
    We investigate this using a dataset we curated from Reddit's r/AmItheAsshole,
    comprising real-world moral dilemmas with crowd-sourced verdicts. Through experiments
    on five state-of-the-art LLMs across 847 posts, we find a significant and systematic
    divergence where LLMs are more lenient than humans. Moreover, we find that translating
    the posts into another language changes LLMs' verdicts, indicating their judgments
    lack cross-lingual stability.
  archival: true
  attributes:
    paper_length: Extended Abstract (2 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@ugent.be'
    first_name: Nan
    google_scholar_id: https://scholar.google.com/citations?user=kCwzupoAAAAJ&hl=en
    homepage: https://research.ugent.be/web/person/nan-li-0/en
    institution: Universiteit Gent
    last_name: Li
    name: Nan Li
    orcid: https://orcid.org/0000-0002-9963-7794
    username: ~Nan_Li4
  - dblp_id: https://dblp.org/pid/26/4533
    emails: '****@ugent.be'
    first_name: Bo
    google_scholar_id: https://scholar.google.com/citations?user=_DxP-KUAAAAJ
    homepage: https://bokang.io
    institution: Ghent University
    last_name: Kang
    name: Bo Kang
    orcid: https://orcid.org/0000-0002-9895-9927
    username: ~Bo_Kang1
  - dblp_id: https://dblp.org/pid/49/2018
    emails: '****@ugent.be'
    first_name: Tijl
    google_scholar_id: https://scholar.google.be/citations?user=eH_c4R4AAAAJ&hl=en
    homepage: http://www.tijldebie.net
    institution: Ghent University
    last_name: De Bie
    name: Tijl De Bie
    orcid: https://orcid.org/0000-0002-2692-7504
    username: ~Tijl_De_Bie1
  decision: '2025'
  file: 18.pdf
  id: 18
  openreview_id: naVVdqKUzO
  pdf_file: 11ddd7b23f6cf848b698c805b7da56085253c94a.pdf
  title: 'Human-AI Moral Judgment Congruence on Real-World Scenarios: A Cross-Lingual
    Analysis'
- abstract: 'The Nüshu script, originating from Jiangyong County, China, is the world’s
    only known writing system historically created and used exclusively by women.
    Although Natural Language Processing (NLP) efforts have begun digitizing limited
    Nüshu-Chinese text pairs, computational access to the script remains highly restricted
    due to its handwritten, visual nature and absence of multimodal tools. We contribute
    two novel datasets: NüshuVision, an image corpus of 500 rendered sentences in
    traditional vertical, right-to-left orthography, and NüshuStrokes, the first sequential
    handwriting recordings of all 397 Unicode Nüshu characters by an expert calligrapher.
    Benchmarking five leading Chinese OCR systems on NüshuVision shows a consistent
    Character Error Rate (CER) of 1.0. Fine-tuning Microsoft’s TrOCR model reduces
    CER to 0.67. These resources mark a crucial step toward multimodal processing
    of Nüshu and present a new paradigm for culturally sensitive language revitalization.'
  archival: false
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/378/1026
    emails: '****@dartmouth.edu'
    first_name: Ivory
    google_scholar_id: https://scholar.google.com/citations?user=WotNdpkAAAAJ&hl=en
    homepage: https://ivoryayang.github.io/
    last_name: Yang
    name: Ivory Yang
    semantic_scholar_id: https://www.semanticscholar.org/author/Ivory-Yang/2303404198
    username: ~Ivory_Yang1
  - dblp_id: https://dblp.org/pid/14/2107
    emails: '****@gmail.com'
    first_name: Xiaobo
    google_scholar_id: https://scholar.google.com/citations?user=z9rwAaIAAAAJ&hl=en
    last_name: Guo
    name: Xiaobo Guo
    orcid: https://orcid.org/0000-0002-6817-626X
    username: ~Xiaobo_Guo1
  - dblp_id: https://dblp.org/pid/68/1041-6
    emails: '****@dartmouth.edu'
    first_name: Yuxin
    google_scholar_id: https://scholar.google.com/citations?user=T2S8PdUAAAAJ
    homepage: https://audreyw.top/AboutMe
    last_name: Wang
    name: Yuxin Wang
    orcid: https://orcid.org/0000-0001-5205-0518
    username: ~Yuxin_Wang7
  - dblp_id: https://dblp.org/pid/351/3944
    emails: '****@dartmouth.edu'
    first_name: Hefan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=wRwevIkAAAAJ
    homepage: https://hf-heaven.github.io/
    institution: Dartmouth College
    last_name: Zhang
    name: Hefan Zhang
    orcid: https://orcid.org/0009-0004-0879-6872
    semantic_scholar_id: https://www.semanticscholar.org/author/Hefan-Zhang/2297675380
    username: ~Hefan_Zhang1
  - dblp_id: https://dblp.org/pid/353/7715
    emails: '****@dartmouth.edu'
    first_name: Yaning
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=OrsUhUQAAAAJ
    homepage: https://www.hust.edu.cn/xxgk/xxjj.htm
    last_name: Jia
    name: Yaning Jia
    username: ~Yaning_Jia2
  - emails: '****@dartmouth.edu'
    first_name: William
    last_name: Dinauer
    name: William Dinauer
    username: ~William_Dinauer2
  - dblp_id: https://dblp.org/pid/01/1709
    emails: '****@dartmouth.edu'
    first_name: Soroush
    google_scholar_id: https://scholar.google.com/citations?user=45DAXkwAAAAJ&hl=en&oi=ao
    homepage: https://www.cs.dartmouth.edu/~soroush/
    institution: Dartmouth College
    last_name: Vosoughi
    name: Soroush Vosoughi
    orcid: https://orcid.org/0000-0002-2564-8909
    semantic_scholar_id: https://www.semanticscholar.org/author/Soroush-Vosoughi/1918441
    username: ~Soroush_Vosoughi1
  decision: '2025'
  file: 19.pdf
  id: 19
  openreview_id: iWALAj6N2C
  pdf_file: 8620ba3a0d931395ab8171fb19842a9c4f0efc63.pdf
  title: Revitalizing Nüshu Through Mixed Media
- abstract: This paper focuses on data-driven dependency parsing for Vedic Sanskrit.
    We propose and evaluate a transfer learning approach that benefits from syntactic
    analysis of typologically related languages, including Ancient Greek and Latin,
    and a descendant language - Classical Sanskrit. Experiments on the Vedic TreeBank
    demonstrate the effectiveness of cross-lingual transfer, demonstrating improvements
    from the biaffine baseline as well as outperforming the current state of the art
    benchmark, the deep contextualised self-training algorithm, across a wide range
    of experimental setups.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@gmail.com'
    first_name: Abhiram
    homepage: https://ramstar3000.github.io/Portfolio/
    last_name: Vinjamuri
    name: Abhiram Vinjamuri
    username: ~Abhiram_Vinjamuri1
  - dblp_id: https://dblp.org/pid/63/6566
    emails: '****@cam.ac.uk'
    first_name: Weiwei
    google_scholar_id: https://scholar.google.com/citations?user=FDtr1fkAAAAJ&hl=en
    homepage: https://www.cl.cam.ac.uk/~ws390/
    institution: University of Cambridge
    last_name: Sun
    name: Weiwei Sun
    username: ~Weiwei_Sun6
  decision: '2025'
  file: 20.pdf
  id: 20
  openreview_id: Iv9zhxakjV
  pdf_file: 0c40ee83d8ad083ff59e08915cfa8e8913f95059.pdf
  title: Transfer learning for dependency parsing of Vedic Sanskrit
- abstract: Political stance detection in low-resource and culturally complex settings
    poses a critical challenge for large language models (LLMs). In the Thai political
    landscape—rich with indirect expressions, polarized figures, and sentiment-stance
    entanglement—LLMs often exhibit systematic biases, including sentiment leakage
    and entity favoritism. These biases not only compromise model fairness but also
    degrade predictive reliability in real-world applications. We introduce ThaiFACTUAL,
    a lightweight, model-agnostic calibration framework that mitigates political bias
    without fine-tuning LLMs. ThaiFACTUAL combines counterfactual data augmentation
    with rationale-based supervision to disentangle sentiment from stance and neutralize
    political preferences. We curate and release the first high-quality Thai political
    stance dataset with stance, sentiment, rationale, and bias markers across diverse
    political entities and events. Our results show that ThaiFACTUAL substantially
    reduces spurious correlations, improves zero-shot generalization, and enhances
    fairness across multiple LLMs. This work underscores the need for culturally grounded
    bias mitigation and offers a scalable blueprint for debiasing LLMs in politically
    sensitive, underrepresented languages.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@student.chula.ac.th'
    first_name: Kasidit
    last_name: Sermsri
    name: Kasidit Sermsri
    username: ~Kasidit_Sermsri1
  - dblp_id: https://dblp.org/pid/201/6162
    emails: '****@gmail.com'
    first_name: Teerapong
    google_scholar_id: https://scholar.google.co.th/citations?user=myy0qDgAAAAJ&hl=en
    homepage: https://kaopanboonyuen.github.io/
    institution: MARSAIL (Motor AI Recognition Solution Artificial Intelligence Laboratory)  and
      Chulalongkorn University
    last_name: Panboonyuen
    name: Teerapong Panboonyuen
    orcid: https://orcid.org/0000-0001-8464-4476
    semantic_scholar_id: https://www.semanticscholar.org/author/Teerapong-Panboonyuen/18098518
    username: ~Teerapong_Panboonyuen1
  decision: '2025'
  file: 21.pdf
  id: 21
  openreview_id: 0svVPSuUVw
  pdf_file: 63d5085309d37d36631a31d8b192aa2bfe1d1763.pdf
  title: Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual
    Calibration
- abstract: Large language models (LLMs) have significantly advanced automated code
    generation and debugging, facilitating powerful multi-agent coding frameworks.
    However, deploying these sophisticated models on resource-constrained edge devices
    remains challenging due to high computational demands, limited adaptability, and
    significant privacy risks associated with cloud-based processing. Motivated by
    these constraints, we propose \textbf{Edge Code Cloak Coder (ECCC)}, a novel edge-cloud
    hybrid framework integrating lightweight quantized LLM with robust AST-based anonymization
    and edge-side privacy validation. ECCC enables high-performance, privacy-preserving
    LLM capabilities on consumer GPUs, anonymizing user code before securely delegating
    abstracted tasks to cloud LLMs. Experimental evaluations demonstrate that ECCC
    achieves competitive correctness (within 4–5pp of the GPT-4-based frameworks)
    and a perfect privacy score of 10/10, effectively balancing functionality and
    security for sensitive and proprietary code applications.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@mail2.sysu.edu.cn'
    first_name: Haoqi
    last_name: He
    name: Haoqi He
    orcid: https://orcid.org/0009-0007-8646-8593
    username: ~Haoqi_He1
  - emails: '****@mail2.sysu.edu.cn'
    first_name: Wenzhi
    homepage: https://azzz-nirvana.github.io
    last_name: Xu
    name: Wenzhi Xu
    username: ~Wenzhi_Xu2
  - emails: '****@mail2.sysu.edu.cn'
    first_name: Ruoying
    homepage: https://yeliaaa.github.io/
    institution: SUN YAT-SEN UNIVERSITY
    last_name: Liu
    name: Ruoying Liu
    username: ~Ruoying_Liu1
  - emails: '****@gmail.com'
    first_name: Jiarui
    institution: Chengdu University
    last_name: Tang
    name: Jiarui Tang
    username: ~Jiarui_Tang1
  - emails: '****@163.com'
    first_name: Bairu
    homepage: https://canvas.gsa.ac.uk/about/18264
    last_name: Li
    name: Bairu Li
    username: ~Bairu_Li1
  - emails: '****@mail2.sysu.edu.cn'
    first_name: Xiaokai
    homepage: https://github.com/D3v3nlxk
    last_name: Lin
    name: Xiaokai Lin
    username: ~Xiaokai_Lin1
  decision: '2025'
  file: 22.pdf
  id: 22
  openreview_id: QG3VIZkNqn
  pdf_file: 56a324752927fbe2f1f39e2c63e9957ab98eaf63.pdf
  title: 'ECCC: Edge Code Cloak Coder for Privacy Code Agent'
- abstract: As AI advances, aligning it with diverse human and societal values grows
    critical. But how do we define these values and measure AI’s adherence to them?
    We present ValueCompass, a framework grounded in psychological theories, to assess
    human-AI alignment. Applying it to five diverse LLMs and 112 humans from seven
    countries across four scenarios—collaborative writing, education, public sectors,
    and healthcare—we uncover key misalignments. For example, humans prioritize national
    security, while LLMs often reject it. Values also shift across contexts, demanding
    scenario-specific alignment strategies. This work advances AI design by mapping
    how systems can better reflect societal ethics.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@uw.edu'
    first_name: Hua
    google_scholar_id: https://scholar.google.com/citations?user=zFjlv1sAAAAJ&hl=en
    homepage: http://hua-shen.org/
    last_name: Shen
    name: Hua Shen
    orcid: https://orcid.org/0000-0002-4928-525X
    semantic_scholar_id: https://www.semanticscholar.org/author/Hua-Shen/145028030
    username: ~Hua_Shen1
  - emails: '****@gmail.com'
    first_name: Tiffany
    google_scholar_id: https://scholar.google.com/citations?user=WXMVE7IAAAAJ&hl=en
    homepage: https://tknearem.wixsite.com/tknearem
    last_name: Knearem
    name: Tiffany Knearem
    orcid: https://orcid.org/0000-0003-4928-6225
    username: ~Tiffany_Knearem1
  - dblp_id: https://dblp.org/pid/324/2458
    emails: '****@microsoft.com'
    first_name: Reshmi
    google_scholar_id: https://scholar.google.com/citations?user=ui8JeF5lKNMC&hl=e
    homepage: https://reshmighosh.github.io
    institution: Microsoft
    last_name: Ghosh
    name: Reshmi Ghosh
    username: ~Reshmi_Ghosh1
  - emails: '****@illinois.edu'
    first_name: Yu-Ju
    last_name: Yang
    name: Yu-Ju Yang
    orcid: https://orcid.org/0009-0005-1989-0044
    username: ~Yu-Ju_Yang1
  - emails: '****@uw.edu'
    first_name: Nicholas
    google_scholar_id: https://scholar.google.com/citations?user=lRikbuYAAAAJ&hl=en
    institution: University of Washington
    last_name: Clark
    name: Nicholas Clark
    semantic_scholar_id: https://www.semanticscholar.org/author/Nicholas-Clark/2283932966
    username: ~Nicholas_Clark2
  - dblp_id: https://dblp.org/pid/38/11520.html
    emails: '****@uw.edu'
    first_name: Tanu
    google_scholar_id: https://scholar.google.com/citations?user=5q_BkVAAAAAJ&hl=en
    homepage: http://tanumitra.com/
    institution: University of Washington
    last_name: Mitra
    name: Tanu Mitra
    username: ~Tanu_Mitra1
  - emails: '****@illinois.edu'
    first_name: Yun
    homepage: https://ischool.illinois.edu/people/yun-huang
    institution: University of Illinois at Urbana-Champaign
    last_name: Huang
    name: Yun Huang
    username: ~Yun_Huang5
  decision: '2025'
  file: 27.pdf
  id: 27
  openreview_id: Cc4QNvkino
  pdf_file: c62a4ceed6007515d8cb90c20f1edc9cf1ccdf10.pdf
  title: 'ValueCompass: A Framework for Measuring Contextual Value Alignment Between
    Human and LLMs'
- abstract: 'We investigate the robustness of Whisper-based automatic speech recognition
    (ASR) models for two major Indonesian regional languages: Javanese and Sundanese.
    While recent work has demonstrated strong ASR performance under clean conditions,
    their effectiveness in noisy environments remains unclear. To address this, we
    experiment with multiple training strategies, including synthetic noise augmentation
    and SpecAugment, and evaluate performance across a range of signal-to-noise ratios
    (SNRs). Our results show that noise-aware training substantially improves robustness,
    particularly for larger Whisper models. A detailed error analysis further reveals
    language-specific challenges, highlighting avenues for future improvements.'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@mbzuai.ac.ae'
    first_name: Salsabila
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=kqXYs4gAAAAJ
    last_name: Pranida
    middle_name: Zahirah
    name: Salsabila Zahirah Pranida
    username: ~Salsabila_Zahirah_Pranida1
  - emails: '****@mbzuai.ac.ae'
    first_name: Rifo
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Genadi
    middle_name: Ahmad
    name: Rifo Ahmad Genadi
    username: ~Rifo_Ahmad_Genadi1
  - dblp_id: https://dblp.org/pid/360/4697
    emails: '****@mbzuai.ac.ae'
    first_name: Muhammad
    google_scholar_id: https://scholar.google.com/citations?user=GXhAThEAAAAJ&hl=en
    homepage: https://sites.google.com/view/mcairlangga/
    last_name: Airlangga
    middle_name: Cendekia
    name: Muhammad Cendekia Airlangga
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhammad-Cendekia-Airlangga/2264465477
    username: ~Muhammad_Cendekia_Airlangga1
  - dblp_id: https://dblp.org/pid/92/6456
    emails: '****@uwaterloo.ca'
    first_name: Shady
    google_scholar_id: https://scholar.google.com/citations?user=osOiYvYAAAAJ&hl=en
    institution: University of Waterloo
    last_name: Shehata
    name: Shady Shehata
    orcid: https://orcid.org/0000-0002-3258-6734
    semantic_scholar_id: https://www.semanticscholar.org/author/Shady-Shehata/38510157
    username: ~Shady_Shehata1
  decision: '2025'
  file: 31.pdf
  id: 31
  openreview_id: JW7F557nbv
  pdf_file: 7bcce177ae747e23c98f65b1ac4d4c6a05023893.pdf
  title: 'ASR Under Noise: Exploring Robustness for Sundanese and Javanese'
- abstract: Scientific visual question answering poses significant challenges for
    vision-language models due to the complexity of scientific figures and their multimodal
    context. Traditional approaches treat the figure and accompanying text (e.g.,
    questions and answer options) as separate inputs. EXAMS-V introduced a new paradigm
    by embedding both visual and textual content into a single image. However, even
    state-of-the-art proprietary models perform poorly on this setup in zero-shot
    settings, underscoring the need for task-specific fine-tuning. To address the
    scarcity of training data in this "text-in-image" format, we synthesize a new
    dataset by converting existing separate image-text pairs into unified images.
    Fine-tuning a small multilingual multimodal model on a mix of our synthetic data
    and EXAMS-V yields notable gains across 13 languages, demonstrating strong average
    improvements and cross-lingual transfer.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@mbzuai.ac.ae'
    first_name: Belal
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=FmCYL00AAAAJ
    last_name: Shoer
    name: Belal Shoer
    username: ~Belal_Shoer1
  - dblp_id: https://dblp.org/pid/225/7708
    emails: '****@mbzuai.ac.ae'
    first_name: Yova
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Kementchedjhieva
    name: Yova Kementchedjhieva
    orcid: https://orcid.org/0009-0000-7465-5702
    semantic_scholar_id: https://www.semanticscholar.org/author/Yova-Kementchedjhieva/51208524
    username: ~Yova_Kementchedjhieva1
  decision: '2025'
  file: 38.pdf
  id: 38
  openreview_id: wEa1EjqsNA
  pdf_file: 3d3915a27f9c3be59a807b8e854d8639be1a6421.pdf
  title: A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA
- abstract: Knowledge distillation (KD) is a popular method of transferring knowledge
    from a large "teacher" model to a small "student" model.  Previous work has explored
    various layer-selection strategies (e.g., forward matching and in-order random
    matching) for intermediate-layer matching in KD, where a student layer is forced
    to resemble a certain teacher layer. In this work, we revisit such layer-selection
    strategies and observe an intriguing phenomenon that layer-selection strategy
    does not matter (much) in intermediate-layer matching---even seemingly nonsensical
    matching strategies such as reverse matching still result in surprisingly good
    student performance. We provide an interpretation for this phenomenon by examining
    the angles between teacher layers viewed from the student's perspective. Our work
    sheds light on KD practice, as layer-selection strategies may not be the main
    focus of KD system design and vanilla forward matching works well in most setups.
  archival: false
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@ualberta.ca'
    first_name: Zony
    institution: University of Alberta
    last_name: Yu
    name: Zony Yu
    username: ~Zony_Yu1
  - dblp_id: https://dblp.org/pid/311/4692
    emails: '****@ualberta.ca'
    first_name: Yuqiao
    homepage: https://apps.ualberta.ca/directory/person/yuqiao
    last_name: Wen
    name: Yuqiao Wen
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuqiao-Wen/2153698023
    username: ~Yuqiao_Wen1
  - emails: '****@gmail.com'
    first_name: Lili
    google_scholar_id: https://scholar.google.com.hk/schhp?hl=en&authuser=1
    homepage: https://lili-mou.github.io/
    institution: University of Alberta
    last_name: Mou
    name: Lili Mou
    username: ~Lili_Mou1
  decision: '2025'
  file: 43.pdf
  id: 43
  openreview_id: GA6qB4E0xp
  pdf_file: 063c1c3eadba438d354700a2f0b7712a670d38db.pdf
  title: 'Revisiting Intermediate-Layer Matching in Knowledge Distillation: Layer-Selection
    Strategy Doesn’t Matter (Much)'
- abstract: 'Large language models (LLMs) excel in generating fluent utterances but
    can lack reliable grounding in verified information. At the same time, knowledge-graph-based
    fact-checkers deliver precise and interpretable evidence, yet suffer from limited
    coverage or latency. By integrating LLMs with knowledge graphs and real-time search
    agents, we introduce a hybrid fact-checking approach that leverages the individual
    strengths of each component. Our system comprises three autonomous steps: 1) a
    Knowledge Graph (KG) Retrieval for rapid one‑hop lookups in DBpedia, 2) an LM-based
    classification guided by a task-specific labeling prompt, producing outputs with
    internal rule-based logic, and 3) a Web Search Agent invoked only when KG coverage
    is insufficient. Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark
    on the Supported/Refuted split without task‑specific fine‑tuning. To address Not
    enough information cases, we conduct a targeted reannotation study showing that
    our approach frequently uncovers valid evidence for claims originally labeled
    as Not Enough Information (NEI), as confirmed by both expert annotators and LLM
    reviewers. With this paper, we present a modular, open-source fact-checking pipeline
    with fallback strategies and generalization across datasets.'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@tum.de'
    first_name: ''
    last_name: Shaghayeghkolli
    name: Shaghayeghkolli
    username: ~Shaghayeghkolli1
  - emails: '****@tum.de'
    first_name: Richard
    last_name: Rosenbaum
    name: Richard Rosenbaum
    orcid: https://orcid.org/0009-0006-0364-388X
    username: ~Richard_Rosenbaum1
  - emails: '****@tum.de'
    first_name: Timo
    last_name: Cavelius
    name: Timo Cavelius
    username: ~Timo_Cavelius1
  - emails: '****@tum.de'
    first_name: Lasse
    last_name: Strothe
    name: Lasse Strothe
    orcid: https://orcid.org/0009-0004-4023-5776
    username: ~Lasse_Strothe1
  - emails: andrii.lata@tum.de
    first_name: Andrii
    institution: NA
    last_name: Lata
    name: Andrii Lata
    username: andrii.lata@tum.de
  - emails: '****@tum.de'
    first_name: Jana
    google_scholar_id: https://scholar.google.com/citations?user=Ct83jG4AAAAJ&hl=en&oi=ao
    homepage: https://www.gov.sot.tum.de/hcc/home/
    institution: Technische Universität München
    last_name: Diesner
    name: Jana Diesner
    orcid: https://orcid.org/0000-0001-8183-7109
    username: ~Jana_Diesner1
  decision: '2025'
  file: 44.pdf
  id: 44
  openreview_id: sMoc9rWHxy
  pdf_file: 71433bdf936842da68d8236f0cb4760508d9aa4f.pdf
  title: Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models,
    and Search-Based Retrieval Agents Improves Interpretable Claim Verification
- abstract: 'Warning: This paper contains explicit statements of offensive stereotypes
    which may be upsetting.


    Stereotypes vary across cultural contexts, making it essential to understand how
    language models encode social biases. MultiLingualCrowsPairs is a dataset of culturally
    adapted stereotypical and anti-stereotypical sentence pairs across nine languages.
    While prior work has primarily reported average fairness metrics on masked language
    models, this paper analyzes social biases in generative models by disaggregating
    results across specific bias types.


    We find that although most languages show an overall preference for stereotypical
    sentences, this masks substantial variation across different types of bias, such
    as gender, religion, and socioeconomic status. Our findings underscore that relying
    solely on aggregated metrics can obscure important patterns, and that fine-grained,
    bias-specific analysis is critical for meaningful fairness evaluation.'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/375/8426
    emails: '****@mi.unc.edu.ar'
    first_name: Guido
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=_QA3Hs4AAAAJ
    institution: Universidad Nacional de Córdoba
    last_name: Ivetta
    name: Guido Ivetta
    orcid: https://orcid.org/0009-0006-3248-1461
    semantic_scholar_id: https://www.semanticscholar.org/author/Guido-Ivetta/2213060824
    username: ~Guido_Ivetta1
  - dblp_id: https://dblp.org/pid/305/6433
    emails: '****@mi.unc.edu.ar'
    first_name: Hernán
    google_scholar_id: https://scholar.google.com/citations?user=wA3vqywAAAAJ&hl=es
    homepage: https://nanom.github.io/
    institution: Universidad Nacional de Córdoba, Argentina
    last_name: Maina
    name: Hernán Maina
    semantic_scholar_id: https://www.semanticscholar.org/author/Hern%C3%A1n-Maina/2139773809
    username: ~Hernán_Maina1
  - dblp_id: https://dblp.org/pid/22/1403
    emails: '****@unc.edu.ar'
    first_name: Luciana
    google_scholar_id: https://scholar.google.com/citations?user=MHvcOG0AAAAJ&hl=es
    homepage: https://benotti.github.io/
    institution: 'Universidad nacional de Córdoba '
    last_name: Benotti
    name: Luciana Benotti
    orcid: https://orcid.org/0000-0001-7456-4333
    semantic_scholar_id: https://www.semanticscholar.org/author/Luciana-Benotti/3131683?sort=pub-date
    username: ~Luciana_Benotti1
  decision: '2025'
  file: 46.pdf
  id: 46
  openreview_id: OLc7pyYQkt
  pdf_file: 2b9b4b4e5da8f59acc5af16c9383c9aa056af017.pdf
  title: Insights from a Disaggregated Analysis of Kinds of Biases in a Multicultural
    Dataset
- abstract: As Large Language Models (LLMs) gain mainstream public usage, understanding
    how users interact with them becomes increasingly important. Limited variety in
    training data raises concerns about LLM reliability across different language
    inputs. To explore this, we test several LLMs using functionally equivalent prompts
    expressed in different English sublanguages. We frame this analysis using Question-Answer
    (QA) pairs, which allow us to detect and evaluate appropriate and anomalous model
    behavior. We contribute a cross-LLM testing method and a new QA dataset  translated
    into AAVE and WAPE variants. Early results reveal a notable drop in accuracy for
    one sublanguage relative to the baseline.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@ufl.edu'
    first_name: William
    institution: University of Florida
    last_name: Coggins
    name: William Coggins
    orcid: https://orcid.org/0009-0009-7850-7489
    username: ~William_Coggins1
  - emails: '****@ufl.edu'
    first_name: Jasmine
    institution: University of Florida
    last_name: McKenzie
    name: Jasmine McKenzie
    orcid: https://orcid.org/0000-0002-9518-6025
    username: ~Jasmine_McKenzie1
  - dblp_id: https://dblp.org/pid/383/0161
    emails: '****@ufl.edu'
    first_name: Sangpil
    google_scholar_id: https://scholar.google.com/citations?user=klSyUtMAAAAJ&hl=en
    homepage: https://nlp.cise.ufl.edu/~youms
    institution: University of Florida
    last_name: Youm
    name: Sangpil Youm
    orcid: https://orcid.org/0000-0001-7234-0395
    semantic_scholar_id: https://www.semanticscholar.org/author/Sangpil-Youm/2189481515
    username: ~Sangpil_Youm1
  - emails: '****@ufl.edu'
    first_name: Pradham
    last_name: Mummaleti
    name: Pradham Mummaleti
    username: ~Pradham_Mummaleti1
  - emails: '****@ufl.edu'
    first_name: Juan
    institution: University of Florida, University of Florida and Clemson University
    last_name: Gilbert
    name: Juan Gilbert
    username: ~Juan_Gilbert1
  - dblp_id: https://dblp.org/pid/69/7691.html
    emails: '****@ufl.edu'
    first_name: Eric
    google_scholar_id: https://scholar.google.com/citations?user=VT7D6oYAAAAJ
    homepage: https://www.cise.ufl.edu/~eragan/
    institution: University of Florida
    last_name: Ragan
    name: Eric Ragan
    orcid: https://orcid.org/0000-0002-7192-3457
    username: ~Eric_Ragan1
  - dblp_id: https://dblp.org/pid/d/BonnieJDorr
    emails: '****@ufl.edu'
    first_name: Bonnie
    homepage: https://www.cise.ufl.edu/dorr-bonnie-j/
    institution: University of Florida
    last_name: Dorr
    middle_name: J
    name: Bonnie J Dorr
    orcid: https://orcid.org/0000-0003-4356-5813
    semantic_scholar_id: https://www.semanticscholar.org/author/B.-Dorr/1752326
    username: ~Bonnie_J_Dorr1
  decision: '2025'
  file: 49.pdf
  id: 49
  openreview_id: 4gyrM0C503
  pdf_file: f1fac8f813f7582043628e60a5fcb8c1822f3381.pdf
  title: 'That Ain''t Right: Assessing LLM Performance on QA in African American and
    West African English Dialects'
- abstract: Understanding emotions is fundamental to many human-computer interaction
    tasks, including customer feedback analysis, marketing insights, and social media
    monitoring. In real-world scenarios, individuals often express multiple emotions
    simultaneously, making multi-label annotation essential for capturing emotional
    complexity. While the EthioEmo dataset (Belay et al., 2025) provides multi-label
    emotion annotations for Ethiopian languages, it lacks information on emotion intensities,
    which are crucial for modeling the strength or subtlety of each emotional expression.
    In this work, we extend the EthioEmo dataset by introducing emotion intensity
    annotations for each labeled emotion, offering a richer and more nuanced resource
    for emotion understanding in low-resource African languages. We evaluate a range
    of encoder-only Pretrained Language Models (PLMs) and open-source Large Language
    Models (LLMs) on this enhanced dataset. Preliminary results show that African-language
    PLMs consistently outperform open-source LLMs, underscoring the importance of
    culturally and linguistically aligned models for emotion analysis in underrepresented
    languages.
  archival: false
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/312/3078
    emails: '****@gmail.com'
    first_name: Tadesse
    google_scholar_id: https://scholar.google.com/citations?user=8S7ilV0AAAAJ&hl=en
    homepage: https://tadesse-destaw.github.io/
    last_name: Belay
    middle_name: Destaw
    name: Tadesse Destaw Belay
    orcid: https://orcid.org/0000-0003-0883-984X
    semantic_scholar_id: https://www.semanticscholar.org/author/Tadesse-Destaw-Belay/1484858320
    username: ~Tadesse_Destaw_Belay1
  decision: '2025'
  file: 51.pdf
  id: 51
  openreview_id: R3M9uGpxMj
  pdf_file: 557127f99aee8dc73c84720624c9c82ec10aa266.pdf
  title: Advancing Emotion Recognition and Intensity Modeling for Ethiopian Languages
- abstract: 'News classification is a downstream task in Natural Language Processing
    (NLP) that involves the automatic categorization of news articles into predefined
    thematic categories. Although notable advancements have been made for high-resource
    languages, low-resource languages such as Amharic continue to encounter significant
    challenges, largely due to the scarcity of annotated corpora and the limited availability
    of language-specific, state-of-the-art model adaptations. To address these limitations,
    this study significantly expands an existing Amharic news dataset, increasing
    its size from 50,000 to 144,000 articles, thus enriching the linguistic and topical
    diversity available for the model training and evaluation. Using this expanded
    dataset, we systematically evaluated the performance of five transformer-based
    models: mBERT, XLM-R, DistilBERT, AfriBERTa, and AfroXLM in the context of Amharic
    news classification. Among these, AfriBERTa and XLM-R achieved the highest F1-scores
    of 90.25\% and 90.11\%, respectively, establishing a new performance baseline
    for the task. These findings underscore the efficacy of advanced multilingual
    and Africa-centric transformer architectures when applied to under-resourced languages,
    and further emphasize the critical importance of large-scale, high-quality datasets
    in enabling robust model generalization. This study offers a robust empirical
    foundation for advancing NLP research in low-resource languages, which remain
    underrepresented in current NLP resources and methodologies.'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@gmail.com'
    first_name: Dagnachew
    last_name: Marilign
    middle_name: Mekonnen
    name: Dagnachew Mekonnen Marilign
    orcid: https://orcid.org/0009-0004-0123-5140
    username: ~Dagnachew_Mekonnen_Marilign1
  - emails: '****@aau.edu.et'
    first_name: Eyob
    google_scholar_id: https://scholar.google.com/citations?user=FH3bvKgAAAAJ&hl=en
    institution: Addis Ababa University
    last_name: Alemu
    middle_name: Nigussie
    name: Eyob Nigussie Alemu
    username: ~Eyob_Nigussie_Alemu1
  decision: '2025'
  file: 52.pdf
  id: 52
  openreview_id: ChZnOITm3K
  pdf_file: 6428eba68931fcffcd207eee615153cf74dd8192.pdf
  title: 'Amharic News Topic Classification: Dataset and Transformer-Based Model Benchmarks'
- abstract: 'This research investigates the detection of covert sales tactics in human-chatbot
    interactions with a focus on the classification of solicited and unsolicited product
    recommendations. A custom dataset of 630 conversations was generated using a Large
    Language Model (LLM) to simulate chatbot-user interactions in various contexts,
    such as when interacting with users from different age groups, recommending different
    types of products and using different types of sales tactics. We then employ various
    approaches, including BiLSTM-based classification with sentence and word-level
    embeddings, as well as zero-shot, few-shot and CoT classification on large state-of-the-art
    LLMs. Our results show that few-shot GPT4 (86.44\%) is the most accurate model
    on our dataset, followed by our compact SBERT+BiLSTM model (78.63\%) - despite
    its small size.

    Our work demonstrates the feasibility of implementing oversight algorithms for
    monitoring chatbot conversations for undesired practices and that such monitoring
    could potentially be implemented locally on-device to mitigate privacy concerns.
    This research thus lays the groundwork for the development of auditing and oversight
    methods for virtual assistants such as chatbots, allowing consumer protection
    agencies to monitor the ethical use of conversational AI.'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@kcl.ac.uk'
    first_name: Simrat
    google_scholar_id: https://scholar.google.com/citations?user=6HzgYOUAAAAJ&hl=en
    last_name: Deol
    name: Simrat Deol
    orcid: https://orcid.org/0000-0002-6785-9691
    username: ~Simrat_Deol1
  - emails: '****@kcl.ac.uk'
    first_name: Jack
    google_scholar_id: https://scholar.google.com/citations?user=0RlziVwAAAAJ&hl=en
    last_name: Contro
    middle_name: Luigi Henry
    name: Jack Luigi Henry Contro
    username: ~Jack_Luigi_Henry_Contro1
  - dblp_id: https://dblp.org/pid/123/6682
    emails: '****@kcl.ac.uk'
    first_name: Martim
    google_scholar_id: https://scholar.google.com/citations?user=zujQKTMAAAAJ
    homepage: https://www.martimbrandao.com/
    institution: King's College London, University of London
    last_name: Brandao
    name: Martim Brandao
    orcid: https://orcid.org/0000-0002-2003-0675
    semantic_scholar_id: https://www.semanticscholar.org/author/Martim-Brandao/143778750
    username: ~Martim_Brandao1
  decision: '2025'
  file: 54.pdf
  id: 54
  openreview_id: yKRurbZL2N
  pdf_file: e1d31d56515780173c5cf03f91ec62a2a10e6796.pdf
  title: Is this Chatbot Trying to Sell Something? Towards Oversight of Chatbot Sales
    Tactics
- abstract: 'Sarcasm is a complex linguistic and pragmatic phenomenon where expressions
    convey meanings that contrast with their literal interpretations, requiring sensitivity
    to the speaker''s intent and context. Misinterpreting sarcasm in collaborative
    human–AI settings can lead to under- or overreliance on LLM outputs, with consequences
    ranging from breakdowns in communication to critical safety failures. We introduce
    Sarc7, a benchmark for fine-grained sarcasm evaluation based on the MUStARD dataset,
    annotated with seven pragmatically defined sarcasm types: self-deprecating, brooding,
    deadpan, polite, obnoxious, raging, and manic. These categories are adapted from
    prior linguistic work and used to create a structured dataset suitable for LLM
    evaluation. For classification, we evaluate multiple prompting strategies—zero-shot,
    few-shot, chain-of-thought (CoT), and a novel emotion-based technique—across five
    major LLMs. Emotion-based prompting yields the highest macro-averaged F1 score
    of 0.3664 (Gemini 2.5), outperforming CoT for several models and demonstrating
    its effectiveness in sarcasm type recognition. For sarcasm generation, we design
    structured prompts using fixed values across four sarcasm-relevant dimensions:
    incongruity, shock value, context dependency, and emotion. Using Claude 3.5 Sonnet,
    this approach produces more subtype-aligned outputs, with human evaluators preferring
    emotion-based generations 38.46% more often than zero-shot baselines. Sarc7 offers
    a foundation for evaluating nuanced sarcasm understanding and controllable generation
    in LLMs, pushing beyond binary classification toward interpretable, emotion-informed
    language modeling.'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@gmail.com'
    first_name: Lang
    last_name: Xiong
    name: Lang Xiong
    username: ~Lang_Xiong2
  - emails: '****@tjhsst.edu'
    first_name: Raina
    homepage: https://www.google.com/
    last_name: Gao
    name: Raina Gao
    username: ~Raina_Gao1
  - emails: '****@gmail.com'
    first_name: Alyssa
    homepage: https://www.google.com/
    last_name: Jeong
    name: Alyssa Jeong
    username: ~Alyssa_Jeong3
  decision: '2025'
  file: 58.pdf
  id: 58
  openreview_id: Uh8GAAbYCZ
  pdf_file: 1bb6cdf782b0a7eb403e7fe0c31f602e84615665.pdf
  title: 'Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and
    Emotion-Informed Techniques'
- abstract: Recent advances in Large Language Models (LLMs) have enhanced the fluency
    and coherence of Conversational Recommendation Systems (CRSs), yet emotional intelligence
    remains a critical gap. In this study, we systematically evaluate the emotional
    behavior of six state-of-the-art LLMs in CRS settings using the ReDial and INSPIRED
    datasets. We propose an emotion-aware evaluation framework incorporating metrics
    such as Emotion Alignment, Emotion Flatness, and per-emotion F1-scores. Our analysis
    shows that most models frequently default to emotionally flat or mismatched responses,
    often misaligning with user affect (e.g., joy misread as neutral). We further
    examine patterns of emotional misalignment and their impact on user-centric qualities
    such as personalization, justification, and satisfaction. Through qualitative
    analysis, we demonstrate that emotionally aligned responses enhance user experience,
    while misalignments lead to loss of trust and relevance. This work highlights
    the need for emotion-aware design in CRS and provides actionable insights for
    improving affective sensitivity in LLM-generated recommendations.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@iiitvadodara.ac.in'
    first_name: Darshna
    institution: Indian Institute of Information Technology, Vadodara
    last_name: Parmar
    name: Darshna Parmar
    orcid: https://orcid.org/0009-0006-0038-6128
    username: ~Darshna_Parmar1
  - dblp_id: https://dblp.org/pid/189/4633.html
    emails: '****@iiitvadodara.ac.in'
    first_name: Pramit
    google_scholar_id: https://scholar.google.com/citations?user=xivnJgQAAAAJ&hl=en
    homepage: http://iiitvadodara.ac.in/faculty-profile/pm001.php
    last_name: Mazumdar
    name: Pramit Mazumdar
    orcid: https://orcid.org/0000-0003-0999-3689
    username: ~Pramit_Mazumdar1
  decision: '2025'
  file: 59.pdf
  id: 59
  openreview_id: 1ITlRtKmwW
  pdf_file: ad623b52f2fe64c750e832b3223d5f36a25c3c26.pdf
  title: Emotionally Aware or Tone-Deaf? Evaluating Emotional Alignment in LLM-Based
    Conversational Recommendation Systems
- abstract: Jailbreaking, the phenomenon where specific prompts cause LLMs to assist
    with harmful requests, remains a critical challenge in NLP, particularly in non-English
    and lower-resourced languages. To address this, we introduce MULBERE, a method
    that extends the method of Targeted Latent Adversarial Training (T-LAT) to a multilingual
    context. We first create and share a multilingual jailbreak dataset spanning high-,
    medium-, and low-resource languages, and then fine-tune LLaMA-2-7b-chat with interleaved
    T-LAT for jailbreak robustness and chat examples for model performance. Our evaluations
    show that MULBERE reduces average multilingual jailbreak success rates by 75\%
    compared to the base LLaMA safety training and 71\% compared to English-only T-LAT
    while maintaining or improving standard LLM performance.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@mit.edu'
    first_name: Anastasia
    last_name: Dunca
    name: Anastasia Dunca
    username: ~Anastasia_Dunca1
  - emails: '****@mit.edu'
    first_name: Maanas
    google_scholar_id: https://scholar.google.co.in/citations?user=KmHKvzsAAAAJ&hl=en&oi=ao
    last_name: Sharma
    middle_name: Kumar
    name: Maanas Kumar Sharma
    username: ~Maanas_Kumar_Sharma1
  - emails: '****@mit.edu'
    first_name: Olivia
    last_name: Munoz
    name: Olivia Munoz
    username: ~Olivia_Munoz1
  - emails: vrosales@mit.edu
    first_name: Victor
    institution: NA
    last_name: Rosales
    name: Victor Rosales
    username: vrosales@mit.edu
  decision: '2025'
  file: 60.pdf
  id: 60
  openreview_id: 6RDRJHm3kv
  pdf_file: 4528429c2d5367b5dcc5dbc3ed1a2e50b29e1cdc.pdf
  title: 'MULBERE: Multilingual Jailbreak Robustness Using Targeted Latent Adversarial
    Training'
- abstract: Language models perpetuate dialect bias, associating African American
    English (AAE) with negative traits and outcomes. We propose JustDial, a lightweight
    finetuning framework aligning character trait associations between meaning-matched
    AAE and Standardized American English (SAE) text, while preserving general model
    fluency through a KL-divergence regularization term. Experiments on GPT2-Medium
    show that JustDial successfully removed any statistically significant correlation
    between dialect and predicted occupational prestige and reduced conviction and
    death-sentencing disparities by more than 98.7\%, with only 100,000 text examples
    and one epoch of LoRA finetuning.  Though this debiasing comes at the cost of
    general model performance, adjusting the regularization term in JustDial enables
    a navigable debiasing-performance tradeoff space. JustDial provides the first
    proof-of-concept towards mitigating dialect prejudice in language models.
  archival: false
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@mit.edu'
    first_name: Maanas
    google_scholar_id: https://scholar.google.co.in/citations?user=KmHKvzsAAAAJ&hl=en&oi=ao
    last_name: Sharma
    middle_name: Kumar
    name: Maanas Kumar Sharma
    username: ~Maanas_Kumar_Sharma1
  - dblp_id: https://dblp.org/pid/237/9060
    emails: '****@mit.edu'
    first_name: Walter
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=5xL774wAAAAJ
    homepage: https://waltergerych.github.io/
    institution: Worcester Polytechnic Institute
    last_name: Gerych
    name: Walter Gerych
    username: ~Walter_Gerych2
  - dblp_id: https://dblp.org/pid/145/6563
    emails: '****@mit.edu'
    first_name: Marzyeh
    homepage: https://www.healthyml.org/
    institution: Massachusetts Institute of Technology
    last_name: Ghassemi
    name: Marzyeh Ghassemi
    username: ~Marzyeh_Ghassemi2
  decision: '2025'
  file: 61.pdf
  id: 61
  openreview_id: 9inWPx4njA
  pdf_file: 63a55889a660502a6d37514adc41170dc5b9e8a7.pdf
  title: 'JustDial: Language Model Dialect Debiasing Using Biased Character Trait
    Associations'
- abstract: 'Capability evaluation of large language models is increasingly shadowed
    by rising concerns of benchmark contamination, which cast doubt on whether static
    legacy benchmarks are measuring genuine reasoning capability or mere memorization.

    Benchmark perturbation emerges as a defining frontier for dynamic evaluation by
    systematically modifying evaluation problems on various levels, such as altering
    numerical values in math problems, modifying narrative contexts in science questions,
    or introducing adversarial triggers. This approach offers a promising method to
    distinguish authentic understanding from pattern matching.

    We propose to study benchmark perturbation in a unified framework where we compare
    the efficacy of various perturbation technique on a collection of legacy benchmarks
    adopted by leading modern developers. We further plan to modularize a variety
    of perturbation techniques to enable plug-and-play refreshing of benchmarks at
    scale for future evaluation. We aim to move beyond simple data collection for
    LLM benchmarking and eventually shift the paradigm towards self-evolving dynamic
    evaluation.'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Thesis Proposal
  authors:
  - dblp_id: https://dblp.org/pid/408/5364
    emails: '****@ethz.ch'
    first_name: Terry
    google_scholar_id: https://scholar.google.co.uk/citations?hl=en&user=hFY4t8gAAAAJ
    homepage: https://terry-zhang.notion.site/
    last_name: Zhang
    middle_name: Jingchen
    name: Terry Jingchen Zhang
    orcid: https://orcid.org/0009-0009-1271-4696
    semantic_scholar_id: https://www.semanticscholar.org/author/Terry-Jingchen-Zhang/2363686592
    username: ~Terry_Jingchen_Zhang1
  decision: '2025'
  file: 62.pdf
  id: 62
  openreview_id: bCL9Xhqohi
  pdf_file: 0548cadfa7e27ccb1dba4266f86755a3abd4259e.pdf
  title: 'Proposal: Revive Legacy Benchmarks By Self-Evolving Perturbations'
- abstract: Multilingual Pre-trained Language Models (multiPLMs) trained with the
    Masked Language Modeling (MLM) objective exhibit suboptimal performance on cross-lingual
    downstream tasks for Low-Resource Languages (LRLs). Continually pre-training these
    multiPLMs with the Translation Language Modeling (TLM) objective on parallel data
    improves the cross-lingual performance. However, both MLM and TLM mask tokens
    randomly, which does not guarantee optimal representation learning. In this paper,
    we introduce a novel masking strategy, Linguistic Entity Masking (LEM) to improve
    the cross-lingual representations of existing multiPLMs. In contrast to MLM and
    TLM, LEM limits masking to the linguistic entities nouns, verbs and Named Entities,
    which hold a higher prominence in a sentence. We hypothesise that masking linguistically
    significant linguistic entities should contribute to effective representation
    learning. Empirically, we prove this using two downstream tasks with three LRL
    pairs, English-Sinhala, English-Tamil, and Sinhala-Tamil, and show that our LEM-based
    learning returns superior results compared to MLM+TLM.
  archival: false
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/277/9520.html
    emails: '****@cse.mrt.ac.lk'
    first_name: Aloka
    google_scholar_id: https://scholar.google.com/citations?user=PDGQ-isAAAAJ&hl=en&authuser=1
    homepage: https://scholar.google.com/citations?user=PDGQ-isAAAAJ&hl=en&authuser=1
    institution: University of Moratuwa
    last_name: Fernando
    name: Aloka Fernando
    orcid: https://orcid.org/0000-0003-0049-4808
    username: ~Aloka_Fernando1
  - dblp_id: https://dblp.uni-trier.de/pid/46/10481.html
    emails: '****@massey.ac.nz'
    first_name: Surangika
    google_scholar_id: https://scholar.google.com/citations?user=TB4RYToAAAAJ&hl=en&oi=ao
    homepage: https://www.massey.ac.nz/massey/expertise/college-staff-lists/college-of-sciences/all-staff/all-staff_home.cfm?stref=319722
    institution: Massey University
    last_name: Ranathunga
    name: Surangika Ranathunga
    orcid: https://orcid.org/0000-0003-0701-0204
    semantic_scholar_id: https://www.semanticscholar.org/author/Surangika-Ranathunga/2594452
    username: ~Surangika_Ranathunga1
  - dblp_id: https://dblp.uni-trier.de/pid/180/9385.html
    emails: '****@cse.mrt.ac.lk'
    first_name: Nisansa
    google_scholar_id: https://scholar.google.com/citations?user=OgPD66oAAAAJ&hl=en
    homepage: http://nisansa.lk
    institution: University of Moratuwa
    last_name: de Silva
    name: Nisansa de Silva
    orcid: https://orcid.org/0000-0002-5361-4810
    semantic_scholar_id: https://www.semanticscholar.org/author/Nisansa-de-Silva/40273954
    username: ~Nisansa_de_Silva1
  decision: '2025'
  file: 63.pdf
  id: 63
  openreview_id: 4sWq2fuPuq
  pdf_file: 36a8ee9a2c4e3b04776a0031bbcb005bb36215e8.pdf
  title: Linguistic Entity Masking to improve Cross-Lingual representations in encoder-based
    LLMs
- abstract: Our desires often influence our beliefs and expectations. Humans tend
    to think good things are more likely to happen than they actually are, while believing
    bad things are less likely. This tendency has been referred to as wishful thinking
    in research on coping strategies. With large language models (LLMs) increasingly
    being considered as computational models of human cognition, we investigate whether
    they can simulate this distinctly human bias. We conducted two systematic experiments
    across multiple LLMs, manipulating outcome desirability and information uncertainty
    across multiple scenarios including probability games, natural disasters, and
    sports events. Our experiments revealed limited wishful thinking in LLMs. In Experiment
    1, only two models showed the bias, and only in sports-related scenarios when
    role-playing characters. Models exhibited no wishful thinking in mathematical
    contexts. Experiment 2 found that explicit prompting about emotional states (being
    hopeful) was necessary to elicit wishful thinking in logical domains. These findings
    reveal a significant gap between human cognitive biases and LLMs' default behavior
    patterns, suggesting that current models require explicit guidance to simulate
    wishful thinking influences on belief formation.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@gmail.com'
    first_name: Nutchanon
    google_scholar_id: https://scholar.google.com/citations?user=k_WXSvMAAAAJ&hl=en
    institution: Thammasat University
    last_name: Yongsatianchot
    name: Nutchanon Yongsatianchot
    username: ~Nutchanon_Yongsatianchot1
  - dblp_id: https://dblp.org/pid/m/StacyMarsella
    emails: '****@gmail.com'
    first_name: Stacy
    google_scholar_id: https://scholar.google.com/citations?user=LkoaA0gAAAAJ
    homepage: http://www.ccis.northeastern.edu/people/stacy-c-marsella/
    institution: Northeastern University
    last_name: Marsella
    name: Stacy Marsella
    username: ~Stacy_Marsella1
  decision: '2025'
  file: 64.pdf
  id: 64
  openreview_id: FQ4HN0Oxac
  pdf_file: d2065d7c02636d429c381133f558d5e33dd5a2ea.pdf
  title: Investigating Motivated Inference in Large Language Models
- abstract: 'We introduce a multilingual benchmark for evaluating large language models
    (LLMs) on hate speech detection and generation in low-resource Ethiopian languages:
    Afaan Oromo, Amharic and Tigrigna, and English (both monolingual and code-mixed).
    Using a balanced and expert-annotated dataset, we assess five state-of-the-art
    LLM families across both tasks. Our results show that while LLMs perform well
    on English detection, their performance on low-resource languages is significantly
    weaker, revealing that increasing model size alone does not ensure multilingual
    robustness. More critically, we find that all models, including closed and open-source
    variants, can be prompted to generate profiled hate speech with minimal resistance.
    These findings underscore the dual risk of exclusion and exploitation: LLMs fail
    to protect low-resource communities while enabling scalable harm against them.
    We make our evaluation framework available to facilitate future research on multilingual
    model safety and ethical robustness.'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/321/9714
    emails: '****@manchester.ac.uk'
    first_name: Nuhu
    google_scholar_id: https://scholar.google.co.uk/citations?user=02ikwAcAAAAJ&hl=en&oi=sra
    homepage: https://personalpages.manchester.ac.uk/staff/nuhu.ibrahim/
    last_name: Ibrahim
    name: Nuhu Ibrahim
    semantic_scholar_id: https://www.semanticscholar.org/author/Nuhu-Ibrahim/2302712378
    username: ~Nuhu_Ibrahim1
  - emails: '****@info-res.org'
    first_name: Felicity
    last_name: Mulford
    name: Felicity Mulford
    username: ~Felicity_Mulford1
  - dblp_id: https://dblp.org/pid/92/11424
    emails: '****@manchester.ac.uk'
    first_name: Riza
    google_scholar_id: https://scholar.google.com/citations?user=fRBJmp9gk_cC&hl=en
    homepage: https://research.manchester.ac.uk/en/persons/riza.batista
    institution: University of Manchester
    last_name: Batista-Navarro
    name: Riza Batista-Navarro
    semantic_scholar_id: https://www.semanticscholar.org/author/R.-Batista-Navarro/1400900759
    username: ~Riza_Batista-Navarro1
  decision: '2025'
  file: 65.pdf
  id: 65
  openreview_id: XXZor39qKM
  pdf_file: 9953bf1643106c767d7628f0ec310b9d9b1e0276.pdf
  title: Large Language Models as Detectors or Instigators of Hate Speech in Low-resource
    Ethiopian Languages
- abstract: We investigate how Vision-Language Models (VLMs) leverage visual features
    when making analogical comparisons about people. Using synthetic images of individuals
    varying in skin tone and nationality, we prompt GPT and Gemini models to make
    analogical associations with desserts and drinks. Results reveal that VLMs systematically
    associate darker-skinned individuals with brown-colored food items, with GPT showing
    stronger associations than Gemini. These patterns are amplified in Thai versus
    English prompts, suggesting language-dependent encoding of visual stereotypes.
    The associations persist across manipulation checks including position swapping
    and clothing changes, though presenting individuals alone yields divergent language-specific
    patterns. This work reveals concerning associations in VLMs' visual reasoning
    that vary by language, with important implications for multilingual deployment.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@gmail.com'
    first_name: Nutchanon
    google_scholar_id: https://scholar.google.com/citations?user=k_WXSvMAAAAJ&hl=en
    institution: Thammasat University
    last_name: Yongsatianchot
    name: Nutchanon Yongsatianchot
    username: ~Nutchanon_Yongsatianchot1
  - emails: '****@brown.edu'
    first_name: Pachaya
    google_scholar_id: https://scholar.google.com/citations?user=ongw6xAAAAAJ&hl=en&oi=ao
    homepage: https://serre-lab.clps.brown.edu/people/
    last_name: Sailamul
    name: Pachaya Sailamul
    orcid: https://orcid.org/0000-0001-6647-870X
    username: ~Pachaya_Sailamul1
  decision: '2025'
  file: 68.pdf
  id: 68
  openreview_id: pU3WKsS6V8
  pdf_file: 541db82d97797b0c4e5efb33efcd2d61d3c0d097.pdf
  title: 'Brown Like Chocolate: How Vision-Language Models Associate Skin Tone with
    Food Colors'
- abstract: 'Multilingual dense embedding models such as Multilingual E5, LaBSE, and
    BGE-M3 have shown promising results on diverse benchmarks for information retrieval
    in low-resource languages. But their result on low resource languages is not up
    to par with other high resource languages. This work improves the performance
    of BGE-M3 through contrastive fine-tuning; the model was selected because of its
    superior performance over other multilingual embedding models across MIRACL, MTEB,
    and SEB benchmarks. To fine-tune this model, we curated a comprehensive dataset
    comprising Yorùbá (32.9k rows), Igbo (18k rows) and Hausa (85k rows) from mainly
    news sources. We further augmented our multilingual dataset with English queries
    and mapped it to each of the Yoruba, Igbo, and Hausa documents, enabling cross-lingual
    semantic training. We evaluate on two settings: the Wura test set and the MIRACL
    benchmark. On Wura, the fine-tuned BGE-M3 raises mean reciprocal rank (MRR) to
    0.9201 for Yorùbá, 0.8638 for Igbo, 0.9230 for Hausa, and 0.8617 for English queries
    matched to local documents, surpassing the BGE-M3 baselines of 0.7846, 0.7566,
    0.8575, and 0.7377, respectively. On MIRACL (Yorùbá subset), the fine-tuned model
    attains 0.5996 MRR, slightly surpassing base BGE-M3 (0.5952) and outperforming
    ML-E5-large (0.5632) and LaBSE (0.4468).'
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@student.funaab.edu.ng'
    first_name: Abdulmatin
    institution: Federal University of Agriculture Abeokutar
    last_name: Omotoso
    name: Abdulmatin Omotoso
    username: ~Abdulmatin_Omotoso1
  - emails: '****@autodesk.com'
    first_name: Habeeb
    homepage: https://habeebshopeju.com
    institution: Autodesk
    last_name: Shopeju
    name: Habeeb Shopeju
    orcid: https://orcid.org/0009-0005-5474-355X
    username: ~Habeeb_Shopeju1
  - emails: '****@gmail.com'
    first_name: Adejumobi
    last_name: Joshua
    middle_name: Monjolaoluwa
    name: Adejumobi Monjolaoluwa Joshua
    username: ~Adejumobi_Monjolaoluwa_Joshua1
  - emails: '****@student.funaab.edu.ng'
    first_name: Shiloh
    last_name: Oni
    name: Shiloh Oni
    username: ~Shiloh_Oni1
  decision: '2025'
  file: 70.pdf
  id: 70
  openreview_id: ghDYqlUu7j
  pdf_file: b4be6b0c6dd4f6aa6763210159cb9270675bd88b.pdf
  title: Improving BGE-M3 Multilingual Dense Embeddings for Nigerian Low Resource
    Languages
- abstract: Pre-trained Chinese Natural Language Processing (NLP) tools show reduced
    performance when analyzing poetry compared to prose. This study investigates the
    discrepancies between tools trained on either Classical or Modern Chinese prose
    when handling Classical Chinese prose and Classical Chinese poetry. Three experiments
    reveal error patterns that indicate the weaker performance on Classical Chinese
    poemsis due to challenges identifying word boundaries. Specifically, tools trained
    on Classical prose struggle recognizing word boundaries within Classical poetic
    structures and tools trained on Modern prose have difficulty with word segmentation
    in both Classical Chinese genres. These findings provide valuable insights into
    the limitations of current NLP tools for studying Classical Chinese literature.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@ufl.edu'
    first_name: Minghao
    google_scholar_id: https://scholar.google.com/citations?user=hZcX774AAAAJ&hl=zh-CN
    last_name: Zheng
    name: Minghao Zheng
    username: ~Minghao_Zheng1
  - dblp_id: https://dblp.org/pid/220/3476
    emails: '****@ufl.edu'
    first_name: Sarah
    google_scholar_id: https://scholar.google.com/citations?user=noq07SMAAAAJ&hl=en
    homepage: https://sarahrmoeller.github.io/
    institution: University of Florida
    last_name: Moeller
    name: Sarah Moeller
    semantic_scholar_id: https://www.semanticscholar.org/author/Sarah-R.-Moeller/39107010
    username: ~Sarah_Moeller1
  decision: '2025'
  file: 71.pdf
  id: 71
  openreview_id: BdG5ti7Z3D
  pdf_file: fded494fbb5be320552abf957adda97f989d3d24.pdf
  title: Challenges in Processing Chinese Texts Across Genres and Eras
- abstract: Sandhi, the phonological merging of morphemes, is a central feature of
    Sanskrit grammar. While Sandhi formation is well-defined by Pāṇini’s Aṣṭādhyāyī,
    the reverse task—Sandhi splitting—is substantially more complex due to inherent
    ambiguity and context-sensitive transformations. Accurate splitting is a critical
    precursor to tokenization in Sanskrit, which lacks explicit word boundaries and
    presents densely fused compounds. In this work, we present a data-driven approach,
    fine-tuning the Gemma-3 4B large language model on a dataset of over 49,000 training
    and 2,000 test examples of compound words and their morpheme-level decompositions.
    Leveraging the Unsloth framework with low-rank adaptation (LoRA) and 4-bit quantization,
    we train the model to predict these splits. Our work yields a scalable, Sandhi-aware
    system designed to enhance modern NLP pipelines for classical Sanskrit, demonstrating
    an effective application of LLMs to this linguistic challenge.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@pesu.pes.edu'
    first_name: Samarth
    last_name: P
    name: Samarth P
    username: ~Samarth_P1
  - emails: '****@pesu.pes.edu'
    first_name: Sanjay
    last_name: Mahalingam
    middle_name: Balaji
    name: Sanjay Balaji Mahalingam
    username: ~Sanjay_Balaji_Mahalingam1
  decision: '2025'
  file: 73.pdf
  id: 73
  openreview_id: TizCCeD6NH
  pdf_file: 896c325c3bd37eb3a261d220bdbadafe682b9c31.pdf
  title: 'The Gemma Sutras: Fine-Tuning Gemma 3 for Sanskrit Sandhi Splitting'
- abstract: 'Large Language Models (LLMs) powered with argentic capabilities are able
    to do knowledge-intensive tasks without human involvement. A prime example of
    this tool is Deep research with the capability to browse the web, extract information
    and generate multi-page reports.

    In this work, we introduce an evaluation sheet that can be used for assessing
    the capability of Deep Research tools. In addition, we selected academic survey
    writing as a use case task and evaluated output reports based on the evaluation
    sheet we introduced. Our findings show the need to have carefully crafted evaluation
    standards. The evaluation done on OpenAI‘s Deep Search and Google’s Deep Search
    in generating an academic survey showed the huge gap between search engines and
    standalone Deep Research tools, as well as the shortcomings in representing the
    targeted area.'
  archival: true
  attributes:
    paper_length: Extended Abstract (2 pages)
    submission_type: Research Paper
  authors:
  - emails: '****@cg.uni-saarland.de'
    first_name: Israel
    homepage: http://israelabebe.github.io/
    last_name: Azime
    middle_name: Abebe
    name: Israel Abebe Azime
    orcid: https://orcid.org/0000-0001-9063-1137
    username: ~Israel_Abebe_Azime1
  - dblp_id: https://dblp.org/pid/312/3078
    emails: '****@gmail.com'
    first_name: Tadesse
    google_scholar_id: https://scholar.google.com/citations?user=8S7ilV0AAAAJ&hl=en
    homepage: https://tadesse-destaw.github.io/
    last_name: Belay
    middle_name: Destaw
    name: Tadesse Destaw Belay
    orcid: https://orcid.org/0000-0003-0883-984X
    semantic_scholar_id: https://www.semanticscholar.org/author/Tadesse-Destaw-Belay/1484858320
    username: ~Tadesse_Destaw_Belay1
  - dblp_id: https://dblp.org/pid/312/3167
    emails: '****@gmail.com'
    first_name: Atnafu
    google_scholar_id: https://scholar.google.com.mx/citations?user=rubyApkAAAAJ&hl=en
    homepage: http://atnafuatx.github.io/
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Tonja
    middle_name: Lambebo
    name: Atnafu Lambebo Tonja
    orcid: https://orcid.org/0000-0002-3501-5136
    semantic_scholar_id: https://www.semanticscholar.org/author/Atnafu-Lambebo-Tonja/2148631756
    username: ~Atnafu_Lambebo_Tonja1
  decision: '2025'
  file: 75.pdf
  id: 75
  openreview_id: WOf8NW4kG8
  pdf_file: 7bd89bee9150d2e997eff5288868d46de81e0a71.pdf
  title: 'Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing'
- abstract: The emergence of Large Language Models (LLMs) as chat assistants capable
    of generating human-like conversations has amplified the need for robust evaluation
    methods, particularly for open-ended tasks. Conventional metrics such as EM and
    F1, while useful, are inadequate for capturing the full semantics and contextual
    depth of such generative outputs. We propose a reference-guided verdict method
    that automates the evaluation process by leveraging multiple LLMs as judges. Through
    experiments on free-form question-answering tasks, we demonstrate that combining
    multiple models improves the reliability and accuracy of evaluations, especially
    in tasks where a single model may struggle. The results indicate a strong correlation
    with human evaluations, establishing the proposed method as a reliable alternative
    to traditional metrics.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/264/3006
    emails: '****@dal.ca'
    first_name: Sher
    google_scholar_id: https://scholar.google.com/citations?user=oxMDd5sAAAAJ&hl=en
    homepage: https://hypermatrix.cs.dal.ca/people/
    last_name: Badshah
    name: Sher Badshah
    orcid: https://orcid.org/0000-0001-6780-3746
    username: ~Sher_Badshah1
  - dblp_id: https://dblp.org/pid/73/5938
    emails: '****@gmail.com'
    first_name: Hassan
    google_scholar_id: https://scholar.google.de/citations?user=t3BH6NkAAAAJ&hl=en
    homepage: https://hsajjad.github.io/
    institution: Dalhousie University
    last_name: Sajjad
    name: Hassan Sajjad
    orcid: https://orcid.org/0000-0002-8584-6595
    semantic_scholar_id: https://www.semanticscholar.org/author/Hassan-Sajjad/145775792
    username: ~Hassan_Sajjad1
  decision: '2025'
  file: 80.pdf
  id: 80
  openreview_id: umU4bccmd4
  pdf_file: fb3ffb1b0b76ccdc4e76d4c92cec24d2755db40a.pdf
  title: 'Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form
    QA'
- abstract: Natural language explanations (NLEs) are widely used to communicate model
    reasoning to humans, but they may also serve as effective control signals for
    improving model performance. In this paper, we present the first comprehensive
    evaluation of NLEs as prompts in in-context learning (ICL), comparing human-annotated,
    self-generated, and LLM-generated NLEs across five reasoning benchmarks and three
    instruction-tuned models (Llama 3 8B, Llama 3 70B, GPT-4o-mini). Our preliminary
    results show that  LLM-generated explanations, especially those from GPT-4o-mini,
    yield the highest gains across tasks. We further plan to measure how the faithfulness
    of self-explanations strongly correlates to its utility, and if models retain
    partial robustness even when rationales are randomly mismatched or adversarially
    swapped.
  archival: false
  attributes:
    paper_length: Extended Abstract (2 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/292/2376
    emails: '****@tum.de'
    first_name: Mahdi
    google_scholar_id: https://scholar.google.com/citations?user=JifHTiwAAAAJ&hl=en
    homepage: https://wwwmatthes.in.tum.de/pages/k2uu16iwws0y/Mahdi-Dhaini
    last_name: Dhaini
    name: Mahdi Dhaini
    orcid: https://orcid.org/0000-0002-7831-3141
    semantic_scholar_id: https://www.semanticscholar.org/author/Mahdi-Dhaini/2240530810
    username: ~Mahdi_Dhaini1
  - dblp_id: https://dblp.org/pid/294/2869
    emails: '****@imperial.ac.uk'
    first_name: Adam
    google_scholar_id: https://scholar.google.com/citations?user=zgX6bw0AAAAJ
    homepage: https://profiles.imperial.ac.uk/adam.dejl18
    institution: Imperial College London
    last_name: Dejl
    name: Adam Dejl
    orcid: https://orcid.org/0009-0006-0274-4160
    semantic_scholar_id: https://www.semanticscholar.org/author/Adam-Dejl/2092514133
    username: ~Adam_Dejl1
  - dblp_id: https://dblp.org/pid/242/4726
    emails: '****@tum.de'
    first_name: Juraj
    google_scholar_id: https://scholar.google.com/citations?user=h7dD6rsAAAAJ&hl=en
    institution: Technische Universität München
    last_name: Vladika
    name: Juraj Vladika
    semantic_scholar_id: https://www.semanticscholar.org/author/Juraj-Vladika/2066962303
    username: ~Juraj_Vladika1
  - emails: '****@tum.de'
    first_name: Volkan
    institution: Technical University of Munich
    last_name: Özer
    name: Volkan Özer
    username: ~Volkan_Özer1
  - dblp_id: https://dblp.org/pid/69/3216
    emails: '****@tum.de'
    first_name: Gjergji
    google_scholar_id: https://scholar.google.com/citations?user=Zbc8GK4AAAAJ&hl=de&oi=sra
    homepage: https://www.gov.sot.tum.de/rds/prof-dr-gjergji-kasneci/
    institution: Technische Universität München and University of Tuebingen
    last_name: Kasneci
    name: Gjergji Kasneci
    orcid: https://orcid.org/0000-0002-3123-7268
    semantic_scholar_id: https://www.semanticscholar.org/author/Gjergji-Kasneci/1686448
    username: ~Gjergji_Kasneci2
  decision: '2025'
  file: 81.pdf
  id: 81
  openreview_id: cSsGjQQXmF
  pdf_file: 6c548ab9607f9473be6a06e4df2af67240979345.pdf
  title: A Comparative Study on the Utility of Natural Language explanations for Enhancing
    Language Models Reasoning Performance
- abstract: Large language models (LLMs) are increasingly integrated into our daily
    lives and personalized. However, LLM personalization might also increase unintended
    side effects. Recent work suggests that persona prompting can lead models to falsely
    refuse user requests. However, no work has fully quantified the extent of this
    issue. To address this gap, we measure the impact of 15 sociodemographic personas
    (based on gender, race, religion, and disability) on false refusal. To control
    for other factors, we also test 16 different models, 3 tasks (Natural Language
    Inference, politeness, and offensiveness classification), and nine prompt paraphrases.
    We propose a Monte Carlo-based method to quantify this issue in a sample-efficient
    manner. Our results show that as models become more capable, personas impact the
    refusal rate less. However, we find that the choice of model significantly influence
    false refusals, especially in sensitive content tasks. The impact of certain sociodemographic
    personas further increases the false refusal effect in some models, which suggests
    that there are underlying biases in the alignment strategies or safety mechanisms.
  archival: true
  attributes:
    paper_length: Short (4 pages)
    submission_type: Research Paper
  authors:
  - dblp_id: https://dblp.org/pid/185/4247.html
    emails: '****@liacs.leidenuniv.nl'
    first_name: Flor
    google_scholar_id: https://scholar.google.com/citations?user=4GqDwGEAAAAJ&hl=en
    homepage: https://fmplaza.github.io/
    institution: Leiden University
    last_name: Plaza-del-Arco
    middle_name: Miriam
    name: Flor Miriam Plaza-del-Arco
    orcid: https://orcid.org/0000-0002-3020-5512
    semantic_scholar_id: https://www.semanticscholar.org/author/Flor-Miriam-Plaza-del-Arco/3455118
    username: ~Flor_Miriam_Plaza-del-Arco1
  - dblp_id: https://dblp.org/pid/282/4243
    emails: '****@unibocconi.it'
    first_name: Paul
    google_scholar_id: https://scholar.google.com/citations?user=7rpmd9cAAAAJ&hl=en
    homepage: https://paulrottger.com/
    institution: Bocconi University
    last_name: Röttger
    name: Paul Röttger
    orcid: https://orcid.org/0009-0008-7115-6893
    semantic_scholar_id: https://www.semanticscholar.org/author/Paul-R%C3%B6ttger/2043232919
    username: ~Paul_Röttger2
  - dblp_id: https://dblp.org/pid/295/0198
    emails: '****@google.com'
    first_name: Nino
    google_scholar_id: https://scholar.google.com/citations?user=CG9n26kAAAAJ&hl=en
    homepage: https://ninodimontalcino.github.io/
    institution: Google
    last_name: Scherrer
    name: Nino Scherrer
    semantic_scholar_id: https://www.semanticscholar.org/author/Nino-Scherrer/1742339548
    username: ~Nino_Scherrer1
  - emails: '****@unibocconi.it'
    first_name: Emanuele
    google_scholar_id: https://scholar.google.it/citations?user=S210UlUAAAAJ&hl=it&oi=ao
    institution: Bocconi University
    last_name: Borgonovo
    name: Emanuele Borgonovo
    username: ~Emanuele_Borgonovo1
  - dblp_id: https://dblp.org/pid/59/11340
    emails: '****@tu-clausthal.de'
    first_name: Elmar
    google_scholar_id: https://scholar.google.com/citations?user=oqP0ljAAAAAJ&hl=de&oi=ao
    institution: Helmholtz-Zentrum Dresden-Rossendorf
    last_name: Plischke
    name: Elmar Plischke
    orcid: https://orcid.org/0000-0002-2019-9243
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Plischke/1766555
    username: ~Elmar_Plischke1
  - dblp_id: https://dblp.org/pid/82/8159
    emails: '****@unibocconi.it'
    first_name: Dirk
    google_scholar_id: https://scholar.google.it/citations?user=7xluaTAAAAAJ&hl=en
    homepage: http://www.dirkhovy.com
    institution: Bocconi University
    last_name: Hovy
    name: Dirk Hovy
    orcid: https://orcid.org/0000-0002-4618-3127
    semantic_scholar_id: https://www.semanticscholar.org/author/Dirk-Hovy/2022288
    username: ~Dirk_Hovy2
  decision: '2025'
  file: 82.pdf
  id: 82
  openreview_id: VtCcrjuOLW
  pdf_file: 49705f1fcf87947a4ee2529ac78e4d69d19bc8bc.pdf
  title: 'No for Some, Yes for Others: Persona Prompts and Other Sources of False
    Refusal in Language Models'
